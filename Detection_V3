# Required Libraries and Packages
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping
import numpy as np
import cv2

# Load the custom COCO dataset
train_datasets = [np.load('dataset_1.npy'), np.load('dataset_2.npy'), np.load('dataset_3.npy'), np.load('dataset_4.npy'), np.load('dataset_5.npy')]
train_labels = [np.load('labels_1.npy'), np.load('labels_2.npy'), np.load('labels_3.npy'), np.load('labels_4.npy'), np.load('labels_5.npy')]

# Concatenate all datasets into one
train_data = np.concatenate(train_datasets)
train_labels = np.concatenate(train_labels)

# Load the 5 COCO datasets

#Path to all files
coco_root = "FinalYearProject\COCO Dataset"

#coco_datasets indiciates the final section of the path required
coco_datasets = ["Cat.json", "Dog.json", "Cow.json", "Goat.json", "Sheep.json"]

#coco_train_datasets uses a comprehension list to  produce a valid file path to each COCO dataset.json
coco_train_datasets = [os.path.join(coco_root, f) for f in coco_datasets]



# Load the base model (ResNet50) with pre-trained ImageNet weights
base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))

# Freeze all layers of the base model
base_model.trainable = False

# Add custom head to the base model
model = tf.keras.Sequential([
  base_model,
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(1024, activation='relu'),
  tf.keras.layers.Dense(1024, activation='relu'),
  tf.keras.layers.Dense(5, activation='softmax')
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# TensorBoard for monitoring the training process
tensorboard = TensorBoard(log_dir='logs')

# ModelCheckpoint to save the best model
checkpoint = ModelCheckpoint("faster_rcnn_best.h5", save_best_only=True, verbose=1)

# EarlyStopping to stop training if there's no improvement in the validation loss
early_stopping = EarlyStopping(patience=10, verbose=1)

# Train the model
history=model.fit(train_data, train_labels, batch_size=32, epochs=100, callbacks=[tensorboard, checkpoint, early_stopping], validation_split=0.2)

# Save the final model
model.save("faster_rcnn_final.h5")
